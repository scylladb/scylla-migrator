---
source:
  type: dynamodb
  table: "YOUR_TABLE_NAME"
  # if using DynamoDB local - specify endpoint with http:// prefix
  # Otherwise, if using aws configure, leave commented out and it will use that
  #endpoint:
  #  host: http://dynamo-local-db-source
  #  port: 8000
  #credentials:
  #  accessKey: empty
  #  secretKey: empty
  ##############################################################################
  # scan segments is how many work chunks will be created to scan the source.
  # A good ROT is to divide source table bytes, taken from describe_table
  # command, and divide by 128MB.  Roundup, and use that for both 
  # scanSegments and maxMapTasks.
  ##############################################################################
  scanSegments: 700
  # If DynamoDB is using throughput limits, you can specify a readThroughput limit
  # or a throughputReadPercent.
  #readThroughput: 1000
  #throughputReadPercent: 0.5
  # MaxMapTasks will be the number of partitions work will be broken up into.
  # Can set it equal to scanSegments.  Or to allow multiple concurrent scans by
  # a single task, set scanSegments to some multiple of maxMapTasks.
  maxMapTasks: 700
  streamChanges: false
  # For scyllaDB - Alternator target, you need to specify endpoint URL.
target:
  type: dynamodb
  table: "YOUR_TABLE_NAME"
  # For scyllaDB - Alternator target, you need to specify endpoint URL.
  # If you have multiple workers, you could use /etc/hosts, and add 
  # an entry to each worker - such that each worker node connects to a 
  # different scylla node.  Then use the "scylla" hostname here.
  # You could alternately use the Alternator DNS load balancer, or
  # The easiest and least performant option is just point to a single nodes IP.
  # YOUR-SCYLLA-NODE0-IP scylla
  endpoint:
    host: http://YOUR_ALTERNATOR_INSTANCE
    port: 8000
  credentials:
    accessKey: empty
    secretKey: empty
    #scanSegments: 10000
  maxMapTasks: 1
  streamChanges: false
savepoints:
  # Where should savepoint configurations be stored? This is a path on the host running
  # the Spark driver - usually the Spark master.
  path: /app/savepoints
  # Interval in which savepoints will be created
  intervalSeconds: 300
renames: []
skipTokenRanges: []
validation:
  # Should WRITETIMEs and TTLs be compared?
  compareTimestamps: false
  # What difference should we allow between TTLs?
  ttlToleranceMillis: 60000
  # What difference should we allow between WRITETIMEs?
  writetimeToleranceMillis: 1000
  # How many differences to fetch and print
  failuresToFetch: 100
  # What difference should we allow between floating point numbers?
  floatingPointTolerance: 0.001
  # What difference in ms should we allow between timestamps?
  timestampMsTolerance: 0